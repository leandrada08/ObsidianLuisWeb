

## Arquitectura de Redes Convolucionales (CNN)
Las **arquitecturas de CNN** se estructuran apilando varias capas convolucionales seguidas de una capa **ReLU** y capas de **pooling**. Estas arquitecturas incluyen algunas redes icónicas:
### LeNet-5
La **arquitectura LeNet-5**, desarrollada por Yann LeCun en 1998, fue una de las primeras redes convolucionales y es famosa por su uso en la **clasificación de dígitos escritos a mano (MNIST)**. La estructura de LeNet-5 consiste en una serie de capas convolucionales y de **pooling** que reducen gradualmente el tamaño de las características mientras aumentan la complejidad de las mismas. La arquitectura está formada por:

- **Capa de Entrada**: Imágenes de tamaño 32x32, aunque las imágenes de MNIST (28x28) se rellenan con ceros para adaptarse.
- **Capas Convolucionales y Pooling**: Varias capas convolucionales seguidas de capas de pooling promedio, como la **C1** (convolucional) y **S2** (pooling). La característica interesante de las capas de pooling es que cada neurona computa la media de sus entradas, multiplicando el resultado por un coeficiente aprendible y sumando un sesgo, lo cual aumenta la capacidad del modelo.
- **Capas Conectadas**: Al final, las capas completamente conectadas permiten a la red aprender combinaciones de características más complejas y llevar a cabo la clasificación final【49:0†source】.
![[Captura de pantalla 2024-10-07 a las 11.06.50 a. m..png|300]]

### AlexNet
La arquitectura **AlexNet** fue diseñada por Alex Krizhevsky en 2012 y se hizo famosa al ganar el desafío **ImageNet ILSVRC** con un **error del 17%**, mientras que el segundo lugar alcanzó solo el 26%. Es similar a LeNet-5, pero mucho más grande y profunda, con varias capas convolucionales apiladas. AlexNet introdujo algunas innovaciones clave:

- **Normalización de Respuesta Local (LRN)**: Aplicada después de la función de activación ReLU en varias capas, con el objetivo de normalizar las salidas, lo cual fomenta la especialización de diferentes mapas de características.
- **Capas de Regularización**: Uso de técnicas como **dropout** con un 50% de probabilidad para reducir el sobreajuste, además de la **aumentación de datos** para crear variaciones en las imágenes de entrenamiento y mejorar la robustez del modelo【49:5†source】.
![[Captura de pantalla 2024-10-07 a las 11.07.22 a. m..png|300]]
### GoogLeNet e Inception
**GoogLeNet**, desarrollado por Christian Szegedy en 2014, ganó el desafío ILSVRC al reducir la tasa de error por debajo del 7%. Una de las innovaciones más importantes fue el uso de **módulos Inception**, que permitieron apilar muchas capas sin aumentar el número de parámetros. Cada módulo Inception utiliza convoluciones de diferentes tamaños (1x1, 3x3, 5x5) y luego concatena sus salidas, lo cual permite capturar características a diferentes escalas:

- **Módulos Inception**: Combinan múltiples capas convolucionales con diferentes tamaños de kernel y max pooling para extraer características a múltiples niveles.
- **Auxiliary Classifiers**: GoogLeNet también incluyó dos clasificadores auxiliares que ayudaban a combatir el problema del **gradiente desvaneciente**, proporcionando una señal de pérdida adicional durante el entrenamiento【49:1†source】.
![[Captura de pantalla 2024-10-07 a las 11.08.27 a. m..png|500]]
![[Captura de pantalla 2024-10-07 a las 11.09.00 a. m..png|500]]
### ResNet
La arquitectura **ResNet**, desarrollada por Kaiming He y su equipo, ganó el desafío de ILSVRC en 2015 con una **tasa de error inferior al 3.6%**. ResNet introdujo el concepto de **aprendizaje residual**, lo cual permitió construir redes mucho más profundas (hasta **152 capas**) sin los problemas de degradación que afectan a las redes tradicionales:

- **Unidades Residuales**: Cada unidad residual aplica dos capas convolucionales, y luego suma la entrada a la salida de esas capas. Esto permite que el modelo aprenda funciones residuales (`f(x) = h(x) - x`), facilitando la propagación de los gradientes y mejorando la convergencia.
- **Conexiones de Atajo (Skip Connections)**: Utilizadas para permitir el paso directo de la señal de entrada a capas superiores, especialmente cuando la dimensión de las características cambia. Esto mantiene el flujo de información intacto y ayuda a estabilizar el entrenamiento de redes profundas【49:2†source】【49:14†source】.
![[Captura de pantalla 2024-10-07 a las 11.10.07 a. m..png|500]]
![[Captura de pantalla 2024-10-07 a las 11.10.30 a. m..png|500]]
![[Captura de pantalla 2024-10-07 a las 11.10.55 a. m..png|500]]
### Xception
**Xception**, creada por François Chollet en 2016, es una extensión de la arquitectura **Inception**, pero reemplaza los módulos de inception por **convoluciones separables en profundidad**. En lugar de combinar patrones espaciales y patrones entre canales en una sola convolución, Xception los modela por separado:

- **Convoluciones Separable en Profundidad**: Estas convoluciones consisten en dos partes: primero una convolución espacial (para cada mapa de características) y luego una convolución regular de 1x1 para combinar los canales. Esto reduce el número de parámetros y el costo computacional, y tiende a mejorar el rendimiento comparado con las capas convolucionales regulares.
- **Aplicación en Redes Muy Profundas**: Xception utiliza 36 capas convolucionales en profundidad, lo cual le permite capturar patrones complejos y generalizar mejor en tareas de visión por computadora【49:8†source】.
![[Captura de pantalla 2024-10-07 a las 11.13.54 a. m..png]]
### SENet
**Squeeze-and-Excitation Networks (SENet)** fue la arquitectura ganadora en el desafío ILSVRC de 2017. SENet se enfoca en **recalibrar** las características más relevantes a través de un pequeño submodelo llamado **bloque SE**, que realiza la siguiente secuencia:

- **Reducción Global de la Dimensionalidad**: Un paso de "promedio global" que reduce cada mapa de características a una sola activación.
- **Bloque de Squeeze y Excitación**: Reduce la dimensionalidad de las características y luego recalibra cada mapa de características de acuerdo con su importancia. Esto ayuda a enfatizar las características importantes y reducir el ruido de las características irrelevantes【49:7†source】.
![[Captura de pantalla 2024-10-07 a las 11.15.00 a. m..png|500]]

