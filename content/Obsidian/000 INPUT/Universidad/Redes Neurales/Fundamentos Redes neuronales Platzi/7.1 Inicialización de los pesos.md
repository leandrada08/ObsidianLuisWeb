

## Importancia de la Inicialización de Pesos

La **inicialización de los pesos** de la red neuronal es crucial para garantizar una **convergencia rápida y estable** durante el entrenamiento. Una inicialización inapropiada puede dar lugar a problemas como **gradientes vanishing** o **gradientes exploding**, que dificultan el aprendizaje.

## Métodos de Inicialización

### Inicialización en Ceros

- **Descripción**: Todos los pesos se inicializan a cero.
- **Problema**: Los gradientes serán idénticos para todas las neuronas de una capa, haciendo que la red pierda capacidad de aprender características diferenciadas, ya que todas las neuronas se actualizarán de la misma manera.

### Inicialización Aleatoria (Distribución Uniforme/Gaussiana)

- Los pesos se inicializan con valores aleatorios generados a partir de una **distribución uniforme o normal**. Esta técnica asegura que los pesos comiencen con valores diferentes, permitiendo una **simetría rota**.
- Sin embargo, la varianza de los pesos puede ser demasiado alta o baja, lo cual puede llevar a que las activaciones de las neuronas se saturen o se mantengan en cero.

### Inicialización Xavier (Glorot)

- **Descripción**: Este método intenta mantener la **varianza de las activaciones constante** a lo largo de todas las capas, y es particularmente útil para redes que utilizan funciones de activación como **Tanh** o **Sigmoid**.
  
  $$
  W^{(l)} \sim \mathcal{U}\left(-\frac{\sqrt{6}}{\sqrt{n_{\text{in}} + n_{\text{out}}}}, \frac{\sqrt{6}}{\sqrt{n_{\text{in}} + n_{\text{out}}}}\right)
  $$

  Donde \(n_{\text{in}}\) y \(n_{\text{out}}\) son los números de neuronas en las capas de entrada y salida, respectivamente.

- **Ventajas**: Reduce los problemas de **vanishing/exploding gradients** en redes con funciones de activación simétricas.

### Inicialización He

- **Descripción**: Diseñada para **funciones ReLU** y variantes, la inicialización **He** se basa en la siguiente fórmula:

  $$
  W^{(l)} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
  $$

  Donde \(n_{\text{in}}\) es el número de neuronas en la capa de entrada.

- **Ventajas**: Mejora el rendimiento al evitar que los valores de las activaciones se saturen o queden en cero, especialmente para redes profundas.

## Comparación de Métodos

- **Inicialización en Ceros** no es útil para redes profundas debido a la pérdida de capacidad de aprendizaje.
- **Inicialización Aleatoria** puede funcionar, pero depende mucho de la escala de los valores iniciales.
- **Xavier** y **He** son métodos más avanzados que se han diseñado para lidiar específicamente con los problemas que ocurren durante la propagación de los gradientes, y se recomiendan generalmente para redes profundas.

---

