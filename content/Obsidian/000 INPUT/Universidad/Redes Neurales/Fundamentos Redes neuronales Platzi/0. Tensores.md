

## ¿Qué es un Tensor?

Un **tensor** es una generalización de los conceptos de **escalar**, **vector**, y **matriz** a dimensiones más altas. Formalmente, un tensor es un objeto matemático que puede representarse mediante una estructura de datos multidimensional.

- **Escalar (Orden 0)**: Un número simple, por ejemplo, $a \in \mathbb{R}$.
- **Vector (Orden 1)**: Una lista de números, por ejemplo, $\mathbf{v} = (v_1, v_2, \ldots, v_n)$.
- **Matriz (Orden 2)**: Una tabla de números organizada en filas y columnas, por ejemplo, $A = (a_{ij})$.
- **Tensor (Orden $n$)**: Una generalización a más dimensiones. Un tensor de orden 3 podría ser representado como una "caja" de valores $T = (t_{ijk})$.

En general, un tensor de orden $n$ tiene componentes que se pueden indexar por $n$ índices: $T_{i_1, i_2, \ldots, i_n}$.

### Notación Matemática de Tensores

- Un tensor se denota por $T$.
- El valor en la posición \((i_1, i_2, \ldots, i_n)\) se denota por $T_{i_1, i_2, \ldots, i_n}$.
- La **dimensión** de un tensor se refiere al tamaño en cada una de sus direcciones, por ejemplo, una matriz de $3 \times 4$ tiene dimensión $(3, 4)$.

## Operaciones con Tensores

### 1. **Suma de Tensores**
La **suma** de dos tensores del mismo orden y dimensión se realiza sumando cada uno de los elementos correspondientes:

$$
S_{i_1, i_2, \ldots, i_n} = A_{i_1, i_2, \ldots, i_n} + B_{i_1, i_2, \ldots, i_n}
$$

### 2. **Producto por un Escalar**
El **producto por un escalar** de un tensor consiste en multiplicar cada componente del tensor por el escalar $\alpha$:

$$
S_{i_1, i_2, \ldots, i_n} = \alpha \cdot T_{i_1, i_2, \ldots, i_n}
$$

### 3. **Producto de Tensores**
Existen varios tipos de **producto** para tensores, como:
- **Producto Punto (Dot Product)**: Similar al producto escalar, este producto reduce el orden del tensor.
- **Producto Exterior (Outer Product)**: Eleva el orden del tensor al combinar dos tensores.

### 4. **Contracción de Tensores**
La **contracción** de un tensor es una operación que reduce el orden al sumar elementos a lo largo de dos índices, como la traza de una matriz.

## Propiedades Matemáticas de los Tensores

- **Linealidad**: Las operaciones sobre tensores son lineales.
- **Cambio de Base**: Los tensores se transforman bajo cambios de coordenadas de forma más general que los vectores y matrices, siendo invariantes bajo estos cambios.

### Tensores en Redes Neuronales

En el contexto de **redes neuronales**, los tensores son fundamentales tanto para almacenar datos como para realizar operaciones matemáticas con ellos.

## 1. Almacenamiento de Información

### Tensores de Datos
- En el **aprendizaje profundo**, los datos suelen ser representados por tensores:
  - Un **tensor de imágenes** puede tener dimensiones \((N, H, W, C)\), donde $N$ es el número de imágenes, $H$ y $W$ son la altura y anchura de cada imagen, y $C$ es el número de canales (RGB).
  - Un **tensor de secuencias** usado en redes recurrentes puede tener la forma \((T, N, F)\), donde $T$ es el número de pasos en el tiempo, $N$ es el tamaño del lote (batch), y $F$ es el número de características por paso de tiempo.

### TensorFlow y PyTorch
En frameworks como **TensorFlow** y **PyTorch**, los tensores son las estructuras básicas para almacenar y manipular datos. Los tensores en estos frameworks se asemejan a **arrays** de NumPy pero incluyen características adicionales, como la habilidad de ser ejecutados en **GPU**.

- **TensorFlow** y **PyTorch** permiten realizar operaciones matemáticas complejas en estos tensores, como derivadas y operaciones matriciales, aprovechando el paralelismo de las GPU.

## 2. Manejo de Tensores en Redes Neuronales

### Operaciones Matemáticas en Tensores
Las operaciones matemáticas que se realizan sobre tensores en redes neuronales incluyen:

- **Multiplicaciones de Matrices**: En cada **capa lineal** de una red neuronal, se realiza un producto matricial entre un tensor de entrada y una matriz de pesos:

  $$
  Z = XW + b
  $$

  donde:
  - $X$ es un tensor de entrada.
  - $W$ es una matriz de pesos.
  - $b$ es un vector de sesgo.
  - $Z$ es el resultado del producto, también un tensor.

### Derivación Automática
La **derivación automática** es crucial en el entrenamiento de redes neuronales. Los tensores permiten calcular los **gradientes** de una función objetivo con respecto a sus parámetros usando el **algoritmo de retropropagación** (backpropagation).

- En **PyTorch**, el cálculo automático de gradientes se maneja con la propiedad `requires_grad`. Si se activa en un tensor, las operaciones matemáticas realizadas sobre ese tensor permiten construir un **grafo computacional** que luego se puede diferenciar automáticamente.

### Regla de la Cadena para Tensores
La regla de la cadena se utiliza para calcular derivadas de funciones compuestas, lo cual es clave durante el entrenamiento de redes neuronales para actualizar los pesos. Matemáticamente, si tenemos una función compuesta $f(g(h(x)))$, la derivada de esta función con respecto a $x$ se puede calcular usando:

$$
\frac{d}{dx} f(g(h(x))) = f'(g) \cdot g'(h) \cdot h'(x)
$$

Esta regla se implementa en redes neuronales al calcular las derivadas de los **tensores de pesos** y **sesgos** durante la fase de retropropagación.

## 3. Aplicaciones de los Tensores en Redes Neuronales

### 3.1 Redes Neuronales Convolucionales (CNN)
En **redes convolucionales** (CNN), se usan tensores para representar imágenes, y las **operaciones de convolución** se realizan mediante productos tensoriales.

- En una operación de convolución 2D, un tensor que representa una imagen ($H, W, C$) se "convoluciona" con un **tensor de filtros** ($k_h, k_w, C, n_f$) para generar un **tensor de salida** que representa las características de alto nivel detectadas por los filtros.

### 3.2 Redes Recurrentes (RNN)
En **redes recurrentes** (RNN), los tensores se utilizan para representar **secuencias de datos**. Cada paso en el tiempo se modela como un tensor, y la **conexión recursiva** implica que se propaguen y almacenen datos a lo largo de varias secuencias.

- Las operaciones matriciales necesarias para calcular la salida en cada tiempo se realizan eficientemente utilizando tensores que contienen todos los datos de la secuencia.

### 3.3 Redes Transformadoras (Transformers)
En **transformers**, los tensores se utilizan para representar secuencias y cálculos de **atención**. Los **mecanismos de autoatención** utilizan operaciones tensoriales para calcular las **similitudes** entre diferentes elementos de una secuencia.

- En la capa de autoatención, el **producto tensorial** entre las matrices de consulta ($Q$), claves ($K$), y valores ($V$) permite determinar qué partes de una secuencia deben prestar atención a otras, usando productos escalares y multiplicaciones de matrices.

### 3.4 Optimización y Gradientes
Durante el **entrenamiento** de redes neuronales, los **gradientes** se calculan respecto a una **función de costo** utilizando **tensores**. Esto permite que el proceso de optimización ajuste los parámetros de la red mediante métodos como **descenso de gradiente**:

$$
\theta_{t+1} = \theta_t - \alpha \nabla_\theta J(\theta)
$$

donde $\theta$ son los parámetros (almacenados como tensores), $\alpha$ es la tasa de aprendizaje, y $\nabla_\theta J(\theta)$ es el gradiente de la función de pérdida.



## Tensores en python
**NumPy** es la biblioteca más utilizada en Python para la manipulación de arreglos numéricos y ofrece soporte completo para **vectores, matrices y tensores** de mayor orden.

#### Creación de Tensores con NumPy
Para crear tensores (arreglos) con NumPy, se usa la función `numpy.array()`:

```python
import numpy as np

# Escalar (0 dimensiones)
escalar = np.array(42)

# Vector (1 dimensión)
vector = np.array([1, 2, 3, 4])

# Matriz (2 dimensiones)
matriz = np.array([[1, 2, 3], [4, 5, 6]])

# Tensor de 3 dimensiones
tensor_3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
```

#### **2.2. Operaciones Básicas con Tensores en NumPy**
- **Suma**:

  ```python
  a = np.array([1, 2, 3])
  b = np.array([4, 5, 6])
  suma = a + b  # Resultado: [5, 7, 9]
  ```

- **Producto Elemento a Elemento**:

  ```python
  producto = a * b  # Resultado: [4, 10, 18]
  ```

- **Producto Matricial**:

  ```python
  A = np.array([[1, 2], [3, 4]])
  B = np.array([[5, 6], [7, 8]])
  producto_matricial = np.dot(A, B)
  ```

- **Funciones Matemáticas**:

  ```python
  logaritmo = np.log(a)  # Aplica logaritmo a cada elemento del tensor
  ```

- **Transposición**:

  ```python
  transpuesta = A.T
  ```

NumPy es excelente para manipular tensores en operaciones lineales, y es la base para muchas otras bibliotecas de machine learning y deep learning.

### Manejo de Tensores con PyTorch
**PyTorch** es una de las bibliotecas más populares para la construcción y entrenamiento de modelos de aprendizaje profundo. Al igual que NumPy, proporciona soporte para el manejo de tensores, pero con funcionalidades adicionales que permiten realizar operaciones automáticas de **diferenciación** y GPU.

#### **3.1. Creación de Tensores con PyTorch**
Para trabajar con tensores en **PyTorch**, se utiliza la función `torch.tensor()`:

```python
import torch

# Escalar (0 dimensiones)
escalar = torch.tensor(42)

# Vector (1 dimensión)
vector = torch.tensor([1, 2, 3, 4])

# Matriz (2 dimensiones)
matriz = torch.tensor([[1, 2, 3], [4, 5, 6]])

# Tensor de 3 dimensiones
tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
```

#### Operaciones Básicas con Tensores en PyTorch
PyTorch soporta operaciones muy similares a las de NumPy:

- **Suma**:

  ```python
  a = torch.tensor([1, 2, 3])
  b = torch.tensor([4, 5, 6])
  suma = a + b  # Resultado: tensor([5, 7, 9])
  ```

- **Producto Elemento a Elemento**:

  ```python
  producto = a * b  # Resultado: tensor([4, 10, 18])
  ```

- **Producto Matricial**:

  ```python
  A = torch.tensor([[1, 2], [3, 4]])
  B = torch.tensor([[5, 6], [7, 8]])
  producto_matricial = torch.matmul(A, B)
  ```

- **Funciones Matemáticas**:

  ```python
  logaritmo = torch.log(a.float())  # Se convierte a tipo float si no lo es
  ```

#### **3.3. Tensores y GPU**
Una de las ventajas de PyTorch es que los tensores se pueden mover fácilmente a la **GPU** para aprovechar la computación paralela, especialmente útil para entrenar redes profundas:

```python
# Creando un tensor en la CPU
tensor_cpu = torch.tensor([1, 2, 3])

# Moviendo el tensor a la GPU
if torch.cuda.is_available():
    tensor_gpu = tensor_cpu.to('cuda')
```

#### **3.4. Gradientes y Backpropagation**
PyTorch permite definir tensores con **gradiente habilitado**, lo cual es esencial para la retropropagación en el entrenamiento de redes neuronales:

```python
# Creación de un tensor con gradiente habilitado
w = torch.tensor(1.0, requires_grad=True)

# Realizamos una operación con w
y = w ** 2

# Calculamos el gradiente
y.backward()

# Ahora, w.grad tiene la derivada de y con respecto a w (que es 2 * w)
print(w.grad)  # Resultado: tensor(2.)
```

### **4. Comparación NumPy vs PyTorch**
- **NumPy** se utiliza principalmente para operaciones matemáticas y de álgebra lineal. No tiene soporte nativo para computación en GPU ni para el cálculo de gradientes.
- **PyTorch** está diseñado específicamente para el aprendizaje profundo. Tiene soporte para **cálculo automático de gradientes** (a través de Autograd) y **cómputo en GPU**, lo cual es muy útil para trabajar con redes neuronales.

### **5. Ejemplo Práctico de Uso de Tensores**
Supongamos que queremos calcular la salida de una **red neuronal de una capa** con una entrada de 3 características y una salida de 1 neurona. Podemos definir los pesos y hacer una multiplicación matricial usando PyTorch:

```python
# Definimos un tensor de entrada (vector de características)
x = torch.tensor([1.0, 2.0, 3.0])

# Definimos los pesos de la capa como un tensor
# En este caso, queremos mapear 3 entradas a una sola salida (por lo que es 1 x 3)
W = torch.tensor([0.1, 0.2, 0.3])

# Calculamos la salida (producto escalar entre el vector de entrada y los pesos)
# Añadimos un sesgo (bias) de 0.5
sesgo = 0.5
y = torch.dot(x, W) + sesgo

print(y)  # Resultado: 1.4
```

En este ejemplo, \( y = 0.1 \times 1 + 0.2 \times 2 + 0.3 \times 3 + 0.5 \), lo cual da como resultado 1.4.

### **Resumen**
- Los **tensores** son una generalización de vectores y matrices, utilizados para representar datos complejos en redes neuronales.
- **NumPy** es una herramienta excelente para manipular tensores en operaciones de álgebra lineal, pero no tiene soporte para cálculo en GPU o gradientes automáticos.
- **PyTorch** proporciona una manera fácil de trabajar con tensores en GPU y tiene **soporte completo para backpropagation**, lo cual lo hace ideal para aplicaciones de aprendizaje profundo.
- En **PyTorch**, se pueden crear tensores con `torch.tensor()`, y luego realizar operaciones de álgebra lineal, moverlos a la GPU, y habilitar el cálculo de gradientes para entrenamiento.

### **Pregunta para ti**:
¿Te gustaría implementar una red neuronal sencilla desde cero usando PyTorch, aprovechando el manejo de tensores y las operaciones con gradientes? O, ¿prefieres explorar alguna otra librería para trabajar con tensores, como **TensorFlow**?