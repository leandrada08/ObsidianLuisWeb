## Introducción
### Donde ajusto la red?
![[Captura de pantalla 2024-10-01 a las 3.06.30 p. m..png]]
- Tenemos mucha capas en una red, por lo cual es dificil saber donde ajustar, para evitar tener un trabajo muy largo haciendo este ajusto, se creo el concepto de backpropagation
### Foward propagation y Backpropagation
En una red neuronal, el aprendizaje se logra a través de dos procesos fundamentales: **forward propagation** y **backpropagation**. Estos procesos son esenciales para calcular la salida de la red y para ajustar los pesos de manera que la red aprenda la función deseada a partir de los datos.

- **Forward Propagation**: Se refiere al paso en el cual una entrada se propaga hacia adelante a través de la red para calcular la salida. Este proceso también calcula los valores intermedios necesarios para la retropropagación.

- **Retropropagación** (o **backpropagation**) es un algoritmo de optimización utilizado para entrenar redes neuronales multicapa. Permite calcular el **gradiente de la función de pérdida** respecto a los **pesos** de la red mediante la **regla de la cadena** del cálculo diferencial, de manera eficiente.
	- El propósito de la retropropagación es actualizar los parámetros del modelo (pesos y sesgos) de forma que la **función de pérdida** (es decir, el error entre la salida predicha y la salida real) se minimice. Este proceso se realiza pasando gradientes desde la **capa de salida** hacia atrás, a través de cada capa de la red, de ahí el nombre **retropropagación**.

- Cuando estamos entrenando la red:
	1. Realizamos un foward propagation
	2. Comparamos el valor obteneido con el idea
	3. Aplicacion una funcion de perdida
	4. Aplicamos el Backpropagation
## Foward Propagation
**Forward propagation** implica pasar los datos de entrada a través de las capas de la red para obtener una salida predicha. Este proceso calcula las activaciones de cada neurona en todas las capas.

El proceso de forward propagation puede considerarse como una serie de transformaciones lineales y no lineales. Las transformaciones lineales se logran con los pesos y sesgos de cada capa, y las funciones de activación introducen la no linealidad necesaria para que la red aprenda patrones complejos.

- Este proceso solo sirve para generar una salida con la configuracion actual del sistema

  
### Matemáticas de Forward Propagation
Para una red neuronal con  capas, el proceso matemático de forward propagation puede representarse como:
1. **Capa de Entrada**: Dado un vector de características , se usa como entrada para la primera capa de la red.
2. **Capas Ocultas y de Salida**:
	-  Para cada capa $(l = 1, 2, \dots, L)$:
		- Se calcula la salida ponderada (pre-activación):

$$z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)}$$
- $z^{(l)}$ representa el valor ponderado antes de aplicar la función de activacion de la capa l
- $W^{(l)}$ es la matriz de pesos que onecta la capa $l-1$ con la capa $l$
- $a^{(l-1)}$ son las activaciones de la capa anterior $l-1$
- $b^{(l)}$ es el vector de sesgos para la capa l
- Luego la salida de cada neurona se obtiene aplicando la funcion de activacion a $z^{(l)}$: $$a^{(l)}=f(z^{(l)})$$
3.	**Capa de Salida:** Después de la última capa, se obtiene la salida $\mathbf{a}^{(L)}$, que representa las predicciones de la red.


  

## Backpropagation
El **backpropagation** (retropropagación) es un algoritmo eficiente para calcular el **gradiente de la función de pérdida** con respecto a los parámetros de la red (pesos y sesgos). Utiliza el **principio de la regla de la cadena** para propagar el error hacia atrás desde la salida hasta las capas anteriores, ajustando los parámetros para minimizar el error.

El objetivo del backpropagation es ajustar los pesos de manera que la función de pérdida se minimice para cada muestra del conjunto de entrenamiento. Esto se logra calculando cómo cambia la pérdida con respecto a cada parámetro $(\frac{\partial L}{\partial W})$, y luego aplicando un algoritmo de optimización como el descenso por gradiente para actualizar los pesos en la dirección que minimice la pérdida.

### Derivación Matemática de Backpropagation

1.	**Definir la Función de Pérdida:** Consideremos una función de pérdida $L(\hat{y}, y)$ que mide la diferencia entre la salida predicha $\hat{y}$ y la etiqueta verdadera $y$. Por ejemplo, una función de pérdida de entropía cruzada para clasificación.
2.	**Gradiente de la Capa de Salida:**
	-	Para la capa de salida L, calculamos el gradiente de la función de pérdida con respecto a la salida de la capa:

$$\delta^{(L)} = \frac{\partial L}{\partial \mathbf{a}^{(L)}} \odot f'(\mathbf{z}^{(L)})$$
- Donde $\odot$ denota la multiplicación elemento a elemento y $f'(\mathbf{z}^{(L)})$ es la derivada de la función de activación de la capa de salida.

3.	**Propagación del Error hacia Atrás:**
	-	Para cada capa $(l = L-1, L-2, \dots, 1)$:
	-	Calculamos el gradiente de la pérdida con respecto a la salida ponderada de la capa anterior:

$$\delta^{(l)} = (\mathbf{W}^{(l+1)})^T \delta^{(l+1)} \odot f{\prime}(\mathbf{z}^{(l)})$$

	-	Aquí, $\delta^{(l)}$ representa el gradiente del error que se propaga hacia atrás a través de la red, y se calcula usando los pesos de la siguiente capa y el gradiente de la función de activación.

4.	**Cálculo de los Gradientes de los Pesos y Sesgos:**
	-	Los gradientes de los pesos para la capa l se calculan como:

$$\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \delta^{(l)} (\mathbf{a}^{(l-1)})^T$$

	-	El gradiente de los sesgos para la capa l se calcula como:

$$\frac{\partial L}{\partial \mathbf{b}^{(l)}} = \delta^{(l)}
$$

#### Actualización de los Pesos

Una vez que se han calculado los gradientes para cada parámetro de la red, se utilizan para actualizar los pesos y los sesgos mediante un método de optimización como Descenso por Gradiente:


$$\mathbf{W}^{(l)} := \mathbf{W}^{(l)} - \eta \frac{\partial L}{\partial \mathbf{W}^{(l)}}
$$

$$
\mathbf{b}^{(l)} := \mathbf{b}^{(l)} - \eta \frac{\partial L}{\partial \mathbf{b}^{(l)}}
$$

Donde \eta es la tasa de aprendizaje que determina el tamaño de los pasos para actualizar los parámetros.

## Propiedades y Desafíos de Forward Propagation y Backpropagation

### Problema de Vanishing Gradients

El problema de los gradientes vanishing ocurre cuando los gradientes se vuelven extremadamente pequeños en las capas cercanas a la entrada, lo cual dificulta la actualización efectiva de los pesos. Esto es común en redes profundas que utilizan funciones de activación como Sigmoid o Tanh.

### Problema de Exploding Gradients
El problema de los gradientes exploding ocurre cuando los gradientes se vuelven extremadamente grandes, causando inestabilidad en el entrenamiento. Este problema suele aparecer en redes muy profundas o en redes recurrentes.

### Soluciones Comunes

-	Funciones de Activación: Utilizar funciones como ReLU ayuda a reducir el problema de vanishing gradients, ya que mantiene gradientes constantes en partes positivas de la función.
-	Inicialización de Pesos: La inicialización adecuada (por ejemplo, inicialización Xavier o He) puede ayudar a controlar la propagación de los gradientes.
-	Batch Normalization: Normalizar las salidas de las capas intermedias duran3te el entrenamiento también ayuda a mitigar los problemas de gradientes.
### Ejemplo Simplificado de Retropropagación
Consideremos una red con:
- Una capa de entrada con dos neuronas ($x_1, x_2$).
- Una capa oculta con dos neuronas ($a_1^{(1)}, a_2^{(1)}$).
- Una capa de salida con una neurona ($\hat{y}$).

**Paso Hacia Adelante**:
1. Se calculan las combinaciones lineales en la capa oculta ($z_1^{(1)}, z_2^{(1)}$) y se aplica una función de activación.
2. La salida de la capa oculta se pasa a la capa de salida, y se calcula la salida final $\hat{y}$.

**Paso Hacia Atrás**:
1. Se calcula el **error en la salida** ($\delta^{(2)}$) usando la derivada de la función de pérdida.
2. Se propaga el error a la capa oculta ($\delta^{(1)}$).
3. Se calculan los **gradientes de los pesos** y se actualizan usando el gradiente descendente.


## Referencias
### Notas relacionadas
- **Nota:**[[Derivadas]]
	- **Relacion-Reflexion:** Hacemos uso de la regla de la cadena para funciones compuestas para poder encontrar el gradiente de la funcion anterior para saber como modificarlo y cual modificar










