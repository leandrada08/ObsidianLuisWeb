### Regularización L1 y L2
Los métodos de **regularización L1 y L2** son técnicas comunes para reducir el sobreajuste mediante la **adición de una penalización** a la función de pérdida que depende de los valores de los pesos.
![[Captura de pantalla 2024-10-02 a las 9.57.49 a. m..png]]
#### **Regularización L2 (Ridge Regularization)**
- Añade una penalización proporcional al **cuadrado** de los pesos. 
- La función de pérdida ajustada se ve así:

  $$
  \mathcal{L}_{\text{L2}} = \mathcal{L} + \lambda \sum_{i=1}^n w_i^2
  $$

  Donde:
  - $\mathcal{L}$ es la función de pérdida original.
  - $w_i$ son los pesos del modelo.
  - $\lambda$ es un **hiperparámetro de regularización** que controla la cantidad de penalización. Si $\lambda$ es alto, se da una mayor importancia a reducir el tamaño de los pesos, lo que puede reducir la capacidad de adaptación del modelo a ruidos específicos.

- La regularización L2 tiende a **reducir los pesos** grandes, empujándolos hacia valores cercanos a cero, lo que hace que el modelo sea **más simple** y menos susceptible al sobreajuste.
- Por lo tanto es util cuando se necesita un modelo con mayor *estabilidad*, y donde los pesos se distribuyan de forma homogenea

#### **Regularización L1 (Lasso Regularization)**
- Añade una penalización proporcional al **valor absoluto** de los pesos.
- La función de pérdida ajustada es:

  $$
  \mathcal{L}_{\text{L1}} = \mathcal{L} + \lambda \sum_{i=1}^n |w_i|
  $$

- La regularización L1 tiende a hacer que algunos pesos se **vuelvan exactamente cero**, promoviendo un modelo **más esparso**. Esto también ayuda a realizar una **selección automática de características**, ya que los pesos de características menos importantes tienden a ser eliminados.
- Este metodo sirve para modelos con enos parametros significativos

#### **Comparación L1 vs L2**
- **L1**: Tiende a llevar algunos pesos a **cero**, lo que reduce la complejidad del modelo y favorece la esparsidad.
- **L2**: Reduce la **magnitud de todos los pesos**, pero sin necesariamente llevarlos a cero, lo que reduce la complejidad sin eliminar variables por completo.
