![[Captura de pantalla 2024-10-01 a las 12.31.01 p. m..png|500]]

## Introduccion
Una **función de pérdida** (también conocida como **función de costo**) es una medida de qué tan mal está haciendo el modelo una tarea específica. Se utiliza para **entrenar** redes neuronales, guiando el ajuste de los **pesos y sesgos** para minimizar la discrepancia entre la salida predicha y la esperada.

Matemáticamente, una función de pérdida es una función $\mathcal{L}(\hat{y}, y)$, donde:
- $\hat{y}$ es la predicción del modelo.
- $y$ es el valor real.

Matemáticamente, la función de pérdida calcula un **error escalar** que depende de los pesos y sesgos de la red. Este error es luego minimizado mediante técnicas de optimización, tales como el **descenso por gradiente** y sus variantes, para encontrar los mejores parámetros para la red.

## Tipos de Funciones de Pérdida según el Problema
Las funciones de pérdida dependen del tipo de problema que se está resolviendo. Los dos tipos principales de problemas son **regresión** (predicción de valores continuos) y **clasificación** (asignación de categorías).

### Funciones de Pérdida para Regresión
Las funciones de pérdida de regresión se utilizan cuando la salida es un valor **continuo**. Algunos ejemplos comunes son:

#### **a. Error Cuadrático Medio (MSE - Mean Squared Error)**
Es la función de pérdida más común para problemas de regresión. Se define como:

$$
\mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2
$$

Donde:
- $N$ es el número de muestras.
- $y_i$ es el valor real para la muestra $i$.
- $\hat{y}_i$ es la predicción del modelo para la muestra $i$.

El **MSE** penaliza errores grandes más que errores pequeños debido a la elevación al cuadrado de las diferencias, lo cual puede ser útil cuando se desean evitar grandes desviaciones.
- Su derivada es simple, lo cual ahce que el gradiente sea facil de calcular.
- Es muy sensible a outliers: un error grande tiene un impacto significativo debido al termino cuadratico


#### **b. Error Absoluto Medio (MAE - Mean Absolute Error)**
Esta función calcula el valor absoluto de la diferencia entre la predicción y el valor real:

$$
\mathcal{L}_{\text{MAE}} = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|
$$

A diferencia del MSE, el **MAE** no exagera los errores grandes, por lo que es más robusto ante **outliers** (valores atípicos).

#### **c. Huber Loss**
Es una combinación entre **MSE** y **MAE** y es menos sensible a valores atípicos que el MSE, pero proporciona una función de pérdida diferenciable en todo momento, lo cual ayuda en la optimización:

$$
\mathcal{L}_{\delta}(y, \hat{y}) = 
\begin{cases} 
\frac{1}{2}(y - \hat{y})^2 & \text{si } |y - \hat{y}| \leq \delta \\
\delta |y - \hat{y}| - \frac{1}{2}\delta^2 & \text{si } |y - \hat{y}| > \delta
\end{cases}
$$

Donde $\delta$ es un valor umbral que determina cuándo pasar de MSE a MAE.

### Funciones de Pérdida para Clasificación
Las funciones de pérdida para clasificación se utilizan cuando la salida del modelo es una **categoría** o clase. Aquí se destacan:

#### Entropía Cruzada (Cross-Entropy)
Es la función de pérdida más utilizada para problemas de **clasificación** , mide la distancia entre dos distribuciones de probabilidad: la distribución real de las etiquetas y la distribución predicha por la red. Se define como
$$L=-\sum_{i=1}^Ny_ilog(\hat{y}_i)$$

Para clasificación binaria:

$$
\mathcal{L}_{\text{CE}} = -\frac{1}{N} \sum_{i=1}^N \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

- $y_i$ es el valor real (0 o 1).
- $\hat{y}_i$ es la probabilidad predicha de que la clase sea 1.

Para problemas de **clasificación multiclase**, se utiliza una generalización de la entropía cruzada, que generalmente se aplica después de una capa **softmax** en la salida:

$$
\mathcal{L}_{\text{CE Multi}} = -\sum_{i=1}^N \sum_{c=1}^C y_{i,c} \log(\hat{y}_{i,c})
$$

Donde $C$ es el número de clases, y $y_{i,c}$ es 1 si la muestra $i$ pertenece a la clase $c$, de lo contrario es 0.

- Se utiliza principalmente en clasificacion binaria y multiclase
- Es especialmente efectiva cuando se combinan con activaciones de la capa de salida que generan probabilidades(softmax o sigmoid)

#### Hinge Loss(Perdida de SVM)
La pérdida hinge se usa principalmente en máquinas de vectores de soporte (SVM) y es útil para problemas de clasificación. Está definida como:

$$L = \sum_{i=1}^N \max(0, 1 - y_i \cdot \hat{y}_i)
$$
Donde:
- $y_i$ es la etiqueta real (\pm 1).
- $\hat{y}_i$ es la predicción del modelo.

Características:
- La función penaliza fuertemente cuando una muestra está en el lado incorrecto del hiperplano o está demasiado cerca del margen.

Ventajas:
- Genera márgenes claros entre las clases, lo cual es efectivo para problemas de clasificación lineal.

Cuándo usarla:
- Utilizada en clasificación binaria cuando se busca maximizar el margen entre clases.
- Es más frecuente en SVM que en redes neuronales, aunque puede ser usada para redes que simulen este tipo de comportamiento.
#### Log-Loss (Binary Cross-Entropy)
El **Log-Loss** es una versión específica de la entropía cruzada para problemas de **clasificación binaria**. La idea es similar: penalizar los errores en la predicción de clases asignando costos que dependen de la diferencia entre la probabilidad predicha y el valor real.





## Comparación y Selección de Funciones de Pérdida
La elección de la **función de pérdida** depende del problema a resolver:
-  **Clasificación Binaria**:
	- **Binary Cross-Entropy** (Entropía cruzada binaria) es la mejor opción ya que mide la similitud entre dos distribuciones de probabilidad (la salida de la red y la etiqueta verdadera).
	- Se usa comúnmente con una **función Sigmoid** en la capa de salida para generar probabilidades.

- **Clasificación Multiclase**:
	- **Categorical Cross-Entropy** se usa en combinación con la función **Softmax** para asegurar que las predicciones sean probabilidades normalizadas.
	- Penaliza más fuertemente cuando la probabilidad predicha está lejos de la probabilidad verdadera, lo cual ayuda a mejorar la precisión del modelo.

- **Regresión**:
	- **Mean Squared Error (MSE)** es la función estándar para problemas de **regresión**.
	- **Mean Absolute Error (MAE)** es otra opción que penaliza los errores de manera lineal y es menos sensible a los outliers en comparación con MSE.

- **Maximización del Margen**:
	- **Hinge Loss** se usa cuando se quiere maximizar el margen entre clases. Es ideal en situaciones donde se requiere tener una separación clara entre las clases.




## Problemas y Desafíos en la Selección de Funciones de Pérdida
- **Vanishing Gradients**: Las funciones de pérdida como **MSE** y **Cross-Entropy** pueden sufrir del problema de los **gradientes vanishing** cuando se usan con funciones de activación que también generan gradientes pequeños (e.g., Sigmoid). Esto puede inhibir la correcta propagación de los gradientes hacia capas profundas.
- **Sensibilidad a Outliers**: La **pérdida cuadrática** (MSE) es particularmente sensible a valores atípicos, lo cual puede distorsionar el entrenamiento, haciendo que el modelo intente minimizar los grandes errores individuales.



## Ejemplo Práctico de Cálculo de Pérdida
Consideremos un problema de clasificación binaria con una sola muestra donde:
- El valor real es $y = 1$.
- La probabilidad predicha por la red es $\hat{y} = 0.8$.
Para la **entropía cruzada binaria**, la pérdida sería:
$$
\mathcal{L}_{\text{CE}} = -[y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y})] = -[1 \cdot \log(0.8) + 0 \cdot \log(0.2)] = -\log(0.8) \approx 0.223
$$
Esta pérdida indica qué tan bien la predicción ($\hat{y} = 0.8$) se aproxima al valor real ($y = 1$).