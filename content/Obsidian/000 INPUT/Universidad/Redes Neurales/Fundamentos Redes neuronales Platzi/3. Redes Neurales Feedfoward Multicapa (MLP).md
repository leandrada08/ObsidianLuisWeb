
## Introducción
Las **Redes Neuronales Feedforward Multicapa** (o **Multilayer Perceptron, MLP**) representan una extensión del perceptrón simple al incluir múltiples capas ocultas entre la entrada y la salida. Estas redes son capaces de resolver problemas no lineales, lo que las hace significativamente más poderosas que el perceptrón simple.


## Estructura de una Red Neuronal Multicapa
Una **Red Neuronal Multicapa** está compuesta por:
- **Capa de Entrada**: Recibe los datos de entrada.
- **Capas Ocultas**: Procesan la información intermedia. Aquí es donde ocurre la "magia" del aprendizaje de características no lineales.
- **Capa de Salida**: Proporciona la salida final, que puede ser una predicción o una clasificación.

![[Captura de pantalla 2024-10-01 a las 11.06.16 a. m..png]]
### Capa de Entrada
La **capa de entrada** es simplemente un conjunto de **neuronas que representan las características de los datos de entrada**. Esta solamente recibe los datos de entrada
   - No realiza ningún procesamiento más allá de transferir los valores de entrada a la siguiente capa.
   - Si el modelo tiene $n$ características (features), la capa de entrada tendrá $n$ neuronas.
### Capas Ocultas
Cada neurona de una capa oculta recibe como entradas los valores de la capa anterior, ponderados con sus respectivos pesos. Estas capas son las intermedias, estas son las principales encargadas de hacer la clasificacion.
   - Estas capas son la clave para la **transformación no lineal** de los datos.
   - Cada neurona de esta capa oculta se comporta como un [[2. Perceptron]] ya visto anteriormente realizando una **combinación lineal** de las salidas de la capa anterior, y aplica una **función de activación** sobre esta combinación.
	   - La diferencia es que esta funcion de activacion es no lineal
   - Puede haber una o más capas ocultas, y cada capa puede tener un número arbitrario de neuronas, dependiendo de la complejidad del problema a resolver.


### Capa de Salida
La **capa de salida** toma como entradas los valores de las neuronas de la capa oculta, ponderados nuevamente con un conjunto de pesos ($w_{jk}$). La salida final se calcula como:
   - Proporciona el **resultado final** del modelo.
   - El número de neuronas en la capa de salida depende del problema: una sola neurona para un problema de regresión o para clasificación binaria, o tantas neuronas como clases para problemas de clasificación múltiple.
## Notacion matematica
Una red feedfoward tipica se puede definir matematicamente a traves de una serie de funciones que conectan cada capa: $$z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)}$$
- $z^{(l)}$ representa el valor ponderado antes de aplicar la función de activacion de la capa l
- $W^{(l)}$ es la matriz de pesos que onecta la capa $l-1$ con la capa $l$
- $a^{(l-1)}$ son las activaciones de la capa anterior $l-1$
- $b^{(l)}$ es el vector de sesgos para la capa l

Luego la salida de cada neurona se obtiene aplicando la funcion de activacion a $z^{(l)}$: $$a^{(l)}=f(z^{(l)})$$

### Vectores
El manejo matematico es ente caso deja de ser una simple ecuacion lineal para convertirse en un sistema de ecuaciones, donde se debera manejar estas operaciones como si fueran vectores y la multiplicacion por los pesos se realizara con el producto escalar
![[Captura de pantalla 2024-10-01 a las 11.43.55 a. m..png|500]]
![[Captura de pantalla 2024-10-01 a las 11.45.06 a. m..png|600]]




## Propagación hacia Adelante (Forward Propagation)

Forward Propagation es el proceso mediante el cual se calcula la salida de la red para una entrada dada. La idea es propagar los datos de la entrada hasta la salida, pasando por todas las capas intermedias y aplicando transformaciones mediante los pesos y funciones de activación.

### Proceso de Forward Propagation
1.	**Capa de Entrada:** Se recibe un vector de características $\mathbf{x}$, que se asigna a las activaciones de la capa de entrada.
2.	**Capas Ocultas**: Para cada capa l, se realiza el cálculo:

$$\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$$

Después, se aplica la función de activación:

$$\mathbf{a}^{(l)} = f(\mathbf{z}^{(l)})$$

3.	**Capa de Salida:** Finalmente, se obtiene la salida aplicando las transformaciones pertinentes sobre la última capa oculta. Para tareas de clasificación, la salida se pasa por una función como Softmax para convertir los resultados en probabilidades de cada clase.

## Diferencias Fundamentales entre el Perceptrón y las Redes Feedforward

El Perceptrón Simple es una red con solo una capa de entrada y una capa de salida (sin capas ocultas). Esta estructura es capaz de aprender únicamente fronteras de decisión lineales en el espacio de características, lo cual es insuficiente para muchos problemas reales que requieren aprender relaciones no lineales.

Por otro lado, el MLP resuelve esta limitación mediante la inclusión de capas ocultas con funciones de activación no lineales. Esta adición permite al MLP aproximar cualquier función continua (según el teorema de aproximación universal), lo cual significa que puede aprender una amplia variedad de patrones y relaciones complejas en los datos.















**Notación Matemática**

  

• Una red feedforward típica se puede definir matemáticamente a través de una serie de funciones que conectan cada capa:

  

Donde:

• representa el valor ponderado antes de aplicar la función de activación en la capa .

• es la matriz de pesos que conecta la capa  con la capa .

• son las activaciones de la capa anterior ().

• es el vector de sesgos para la capa .

• Luego, la salida de cada neurona se obtiene aplicando la **función de activación** a :

  

Donde  es una función de activación no lineal, como **ReLU** o **Sigmoid**.

  

**2. Propagación hacia Adelante (Forward Propagation)**

  

**Forward Propagation** es el proceso mediante el cual se calcula la salida de la red para una entrada dada. La idea es propagar los datos de la entrada hasta la salida, pasando por todas las capas intermedias y aplicando transformaciones mediante los pesos y funciones de activación.

  

**Proceso de Forward Propagation**

  

1. **Capa de Entrada**: Se recibe un vector de características , que se asigna a las activaciones de la capa de entrada.

2. **Capas Ocultas**: Para cada capa , se realiza el cálculo:

  

Después, se aplica la función de activación:

  

3. **Capa de Salida**: Finalmente, se obtiene la salida aplicando las transformaciones pertinentes sobre la última capa oculta. Para tareas de clasificación, la salida se pasa por una función como **Softmax** para convertir los resultados en probabilidades de cada clase.

  

**3. Diferencias Fundamentales entre el Perceptrón y las Redes Feedforward**

  

El **Perceptrón Simple** es una red con solo una capa de entrada y una capa de salida (sin capas ocultas). Esta estructura es capaz de aprender únicamente **fronteras de decisión lineales** en el espacio de características, lo cual es insuficiente para muchos problemas reales que requieren aprender relaciones no lineales.

  

Por otro lado, el **MLP** resuelve esta limitación mediante la inclusión de **capas ocultas** con funciones de activación no lineales. Esta adición permite al MLP aproximar cualquier función continua (según el **teorema de aproximación universal**), lo cual significa que puede aprender una amplia variedad de patrones y relaciones complejas en los datos.


  




## Referencias
## Notas relacionadas
- **Nota:**[[Matrices y Sistema de ecuaciones lineales]]
	- **Relacion-Reflexion:** Aqui podemos ver como tenemos que hacer uso de ciertos conceptos de algebra para el manejo de redes neuronales