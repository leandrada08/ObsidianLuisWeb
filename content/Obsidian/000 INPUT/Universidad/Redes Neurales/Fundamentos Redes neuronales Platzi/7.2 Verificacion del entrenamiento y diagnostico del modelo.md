

## Cheques de Gradiente

Los **cheques de gradiente** se utilizan para verificar que la implementación del **algoritmo de backpropagation** sea correcta. Esto es especialmente importante al implementar redes neuronales personalizadas, ya que errores en los cálculos del gradiente pueden llevar a un mal entrenamiento o a la imposibilidad de entrenar.

### Gradiente Numérico

- **Descripción**: Los gradientes se calculan mediante el método de **diferencias finitas**:

  $$
  \frac{\partial L}{\partial \theta} \approx \frac{L(\theta + \epsilon) - L(\theta - \epsilon)}{2\epsilon}
  $$

  Donde \(\epsilon\) es un valor pequeño (e.g., \(10^{-5}\)).

- **Comparación con Backpropagation**: Se comparan los gradientes calculados numéricamente con los gradientes analíticos obtenidos por backpropagation para verificar su exactitud.

## Diagnóstico de Overfitting y Underfitting

### Overfitting

- **Descripción**: Ocurre cuando el modelo se ajusta demasiado a los datos de entrenamiento y no generaliza bien en el conjunto de prueba. Esto se refleja en un **alto rendimiento** en el conjunto de entrenamiento pero **bajo rendimiento** en el conjunto de validación/prueba.
  
- **Soluciones**:
  - **Regularización**: Añadir penalizaciones como L1 o L2.
  - **Dropout**: Introducir dropout durante el entrenamiento.
  - **Más Datos**: Aumentar la cantidad de datos de entrenamiento si es posible.

### Underfitting

- **Descripción**: Sucede cuando el modelo es demasiado simple y no captura la complejidad de los datos. Se refleja en un **bajo rendimiento** tanto en el conjunto de entrenamiento como en el de validación.
  
- **Soluciones**:
  - **Incrementar la Complejidad**: Añadir más capas o neuronas a la red.
  - **Reducir Regularización**: Disminuir la penalización para permitir un ajuste más flexible.

## 3. Monitoreo Durante el Entrenamiento

### Pérdida y Precisión

- Es fundamental **monitorear la pérdida** y la **precisión** durante el entrenamiento para detectar si el modelo está convergiendo adecuadamente. Se pueden trazar curvas de pérdida y precisión para los conjuntos de entrenamiento y validación para entender cómo progresa el entrenamiento.

### Distribución de Activaciones

- **Vanishing y Exploding Activations**: Revisar las activaciones de las neuronas en cada capa para detectar si se están saturando (lo cual indica que podrían sufrir de **vanishing gradients**) o si están creciendo exponencialmente (indicativo de **exploding gradients**).

### Ratios de Pesos respecto a Actualizaciones

- Comparar los **ratios de actualización de los pesos** (\(\frac{\Delta w}{w}\)) durante el entrenamiento puede ayudar a evaluar la efectividad del proceso de aprendizaje. Valores extremadamente altos o bajos pueden indicar problemas de inestabilidad.
