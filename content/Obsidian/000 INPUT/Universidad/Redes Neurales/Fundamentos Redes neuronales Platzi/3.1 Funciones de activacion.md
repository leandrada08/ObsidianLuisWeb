
## Introduccion
Las **funciones de activación** son una parte fundamental de las redes neuronales, ya que introducen la no linealidad necesaria para que una red neuronal pueda aprender y representar funciones complejas. Sin funciones de activación no lineales, una red con múltiples capas se reduciría a una transformación lineal, lo que limitaría severamente su capacidad de aprendizaje.

En una red neuronal, la función de activación se aplica a la salida de cada neurona después de calcular la combinación lineal de las entradas ponderadas. Las funciones de activación comunes incluyen **Sigmoid**, **Tanh**, **ReLU**, **Leaky ReLU** y **Maxout**. Las funciones de activacion pueden ser discretas o continuas.
![[Captura de pantalla 2024-10-07 a las 10.08.29 a. m..png]]
## Propiedades de las funciones de activación
- **Centradas en Cero**:
    - Funciones como **tanh** están centradas en cero, lo que permite una mejor propagación del gradiente y un aprendizaje más estable, ya que la salida tiene valores tanto positivos como negativos.

- **No Lineales**:
    - La **no linealidad** es crucial, ya que permite que la red neuronal modele relaciones complejas. Funciones como **ReLU** o **sigmoid** introducen estas no linealidades que permiten aprender funciones complejas.

- **Tolerante al Gradiente que Desaparece (Vanishing Gradient Tolerant)**:    
    - **ReLU** es una función de activación tolerante al problema del desvanecimiento del gradiente, ya que su derivada es constante para entradas positivas.
    - Funciones como **sigmoid** y **tanh** sufren de desvanecimiento del gradiente cuando sus entradas están en las regiones extremas.

- **Diferenciables**:
    - Todas las funciones de activación utilizadas en redes profundas son **diferenciables**, lo cual es necesario para aplicar el algoritmo de **retropropagación**.

- **Computationally Inexpensive**:    
    - **ReLU** es computacionalmente barata, ya que se define simplemente como el valor máximo entre 0 y la entrada. Esto permite una evaluación rápida y eficiente, especialmente en arquitecturas profundas.

## Funciones de activacion
### Escalon
Es la que se suele utilizar cuando solo puedo tener 2 valores, es la usada en el perceptron
![[Captura de pantalla 2024-10-01 a las 12.03.03 p. m..png|500]]


### Signo
Muy parecida a la escalonada pero incluyendo posibles valores negativos
![[Captura de pantalla 2024-10-01 a las 12.03.43 p. m..png|500]]



### Sigmoide

   $$
   f(z) = \frac{1}{1 + e^{-z}}
   $$

   La sigmoide comprime los valores de salida en el rango (0, 1), lo cual es útil en problemas de **clasificación binaria**. Tiene un inconveniente: su **saturación** cuando los valores son muy altos o muy bajos, haciendo que el gradiente se reduzca mucho (problema conocido como **vanishing gradient**).
   - Se suele usar mucho para probabilidades y clasificacion binaria
   - No se recomienda para capas ocultas debido a los preblemas de saturacion
   - Tambien se utiliza mucho ya que tiene derivadas
   ![[Captura de pantalla 2024-10-01 a las 12.04.49 p. m..png|500]]

### Tangente Hiperbólica (tanh)

   $$
   f(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
   $$

   Produce valores en el rango (-1, 1). Es similar a la sigmoide, pero está centrada en cero, lo cual puede acelerar el aprendizaje, esto sucede ya que reduce la oscilacion de los gradientes.
   - Proporciona una activacion mas fuerte en comparacion con Sigmoid
   - Tambien sufre el problema de *gradientes vanishing*
   - Es recomendable usarla sobre sigmoid en capas ocultas, especialmente cuando se necesita que las activaciones esten centradas en cero
   - Se utiliza en redes recurrentes RNN debido a sus propiedades simetricas
![[Captura de pantalla 2024-10-01 a las 12.05.56 p. m..png|500]]


### ReLU (Rectified Linear Unit)
![[Captura de pantalla 2024-10-01 a las 12.06.46 p. m..png]]

   $$
   f(z) = \max(0, z)
   $$

   ReLU es actualmente la función de activación más popular debido a su **simplicidad** y a que ayuda a mitigar el problema del gradiente desaparecido. Sin embargo, sufre del problema de "neuronas muertas", donde ciertos pesos pueden hacer que la neurona nunca se active.
   - Rango de salida $[0, ∞)$
   - Produce esparsidad en las activaciones, lo cual es util en muchas aplicaciones practicas
   - Es computacionalmente eficiente y facil de implementar
   - Resuelve el problema del gradiente vanishing.
   - Esta funcion basicamente funciona como un Diodo([[Juntura PN]])
- Es la funcion mas utilizada en capac ocultas en redes profundas, especialmente en convolucionales.
- No se recomienda en redes recurrentes debido a la inestabilidad que puede generar en secuencias largas
### Softmax
![[Captura de pantalla 2024-10-01 a las 12.08.40 p. m..png]]
Usada generalmente en la capa de salida para problemas de clasificación

   $$
   f(z_i) = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}
   $$

   Softmax convierte los valores de la capa de salida en **probabilidades** que suman 1, lo cual es útil para la clasificación de múltiples clases.



### Leaky ReLU
-	Definición:
$$f(x) =
\begin{cases}
x & \text{si } x > 0 \\
\alpha x & \text{si } x \leq 0
\end{cases}$$

Donde $\alpha$ es un pequeño valor positivo (típicamente $\alpha = 0.01$).
- Rango de salida: (-∞, ∞)
- Características:
- Permite valores negativos con una pendiente pequeña, evitando que las neuronas se queden completamente inactivas.

Ventajas:
-	Resuelve el problema de neurona muerta de ReLU, ya que siempre hay un gradiente no nulo, incluso para valores negativos.

Desventajas:
- Introduce un pequeño ajuste manual (\alpha), que puede necesitar ajuste.

Cuándo usarla:
-	Útil en situaciones donde el modelo sufre de neuronas muertas al usar ReLU.
-	Buena opción para modelos en los que se observa un problema significativo con la activación nula.

### Maxout
- Definición:
$$f(x) = \max(w_1^T x + b_1, w_2^T x + b_2)$$

Donde $w_1, w_2$ y $b_1, b_2$ son parámetros aprendidos.
- Rango de salida: $(-∞, ∞)$
- Características:
- Aprende la función de activación que maximiza la salida de entre un conjunto de posibles funciones lineales.

Ventajas:
- Puede evitar el problema de gradientes vanishing y gradientes exploding.
- Combina las ventajas de ReLU y le permite adaptarse mejor a los datos.

Desventajas:
- Incrementa significativamente el número de parámetros a aprender.

Cuándo usarla:
- Recomendable cuando se necesita máxima flexibilidad en la selección de la función de activación.
- Es efectiva, pero generalmente se utiliza menos debido a la alta complejidad de los parámetros.

### ELU
![[Captura de pantalla 2024-10-07 a las 10.40.19 a. m..png]]

## Comparación y Selección de Funciones de Activación

La elección de la función de activación depende del tipo de problema y de la capa en la que se aplica. Aquí se describen algunas recomendaciones prácticas:

### Capas Ocultas:
- ReLU es la opción más popular para redes profundas debido a su capacidad de reducir el problema de gradientes vanishing y su simplicidad computacional.
- Leaky ReLU es preferida cuando el problema de neurona muerta es significativo y se necesita que todas las neuronas tengan al menos un gradiente mínimo.
- Tanh puede ser útil si se requiere que las salidas estén centradas en cero, pero se debe tener cuidado con gradientes vanishing.
- Maxout es potente y flexible, pero el costo computacional adicional y el aumento de parámetros la hacen menos atractiva en comparación con otras funciones.

### Capas de Salida:
- Clasificación Binaria: Sigmoid se usa comúnmente para problemas binarios ya que produce una salida probabilística en el rango [0, 1].
- Clasificación Multiclase: Softmax es la función de activación estándar para la capa de salida en problemas de clasificación con múltiples clases exclusivas.
- Regresión: Para tareas de regresión, funciones lineales (sin activación) o activaciones específicas como ReLU se suelen utilizar dependiendo del rango de valores.

### Problemas de Vanishing y Exploding Gradients

- Las funciones Sigmoid y Tanh son susceptibles al problema de los gradientes vanishing, ya que sus derivadas tienden a valores muy pequeños cuando las entradas son grandes o pequeñas, lo cual inhibe el aprendizaje efectivo de los pesos en las capas iniciales de redes profundas.
- ReLU, por otro lado, ayuda a aliviar el problema de los gradientes vanishing, pero puede dar lugar al problema de neurona muerta.
- Elegir adecuadamente la función de activación según la arquitectura y el tipo de problema es crucial para asegurar que los gradientes fluyan efectivamente a través de la red y que el modelo pueda aprender.
## Generando funciones de activacion en Python
- Importo libreria
``` Python
import numpy as np
import matplotlib.pyplot as plt
```

- Defino las funciones de activación que usare
``` Python
def sigmoid(a):
	return 1/(1+np.exp(-a))

def step(a):
	return np.piecewise(a,[a<0.0,a>0.0],[0,1])

def relu(a):
	return np.piecewise(a,[a<0.0,a>0.0],[0,lambda a:a])
```

- Genero el eje de las x
``` Python
x=np.linspace(10,-10,100)
```

- Grafico las diferentes funciones de activacion
``` Python
plt.plot(x,sigmoid(x))
plt.plot(x,step(x))
plt.plot(x,relu(x))
```


![[Captura de pantalla 2024-10-01 a las 12.27.57 p. m..png]]![[Captura de pantalla 2024-10-01 a las 12.28.23 p. m..png]]![[Captura de pantalla 2024-10-01 a las 12.28.40 p. m..png]]




