### Batch Normalization
Batch Normalization normaliza las entradas de cada capa utilizando la media y la varianza de cada mini-lote durante el entrenamiento:

$$\hat{x}i = \frac{x_i - \mu{\text{batch}}}{\sqrt{\sigma_{\text{batch}}^2 + \epsilon}}
$$
Luego, se escala y se desplaza usando parámetros aprendibles $\gamma$ y $\beta:$

$$y_i = \gamma \hat{x}_i + \beta$$

-	Características:
	- Reduce el shifting interno de covarianza, lo cual estabiliza y acelera el entrenamiento.
	- Aporta una regularización adicional que mejora la generalización del modelo.

![[Captura de pantalla 2024-10-03 a las 12.08.15 p. m..png]]
Principalmente solo se aplicaba a la entrada pero luego se comenzo a aplicar a cada capa de nuestra red, ya que los pesos van variando y van desnormalizadno nuestra red.
### Normalización de Entrada
Los datos de entrada se normalizan para que tengan una media cercana a 0 y una varianza de 1. Esto asegura que todas las características estén en la misma escala.
-	Características:
	- Ayuda a que la red converja más rápidamente durante el entrenamiento.
	- Cuándo usarla: Es un paso esencial en el preprocesamiento de datos antes de entrenar cualquier red neuronal.


